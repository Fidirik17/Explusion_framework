{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Specifies a base path in which to save the trained models, XAI explanations, and probabilistic scores for each individual model.\n",
        "I suggest you create three folders, namely: \"models\", \"explainers\", and \"probas\", and define the following paths:\n",
        "*  BASE_PATH/models/probas\n",
        "*  BASE_PATH/explainers\n",
        "\n",
        "(you can also use your google drive)",
        "\n",
        "**Attention: this notebook is compatibile only for binary task!**
      ],
      "metadata": {
        "id": "-s_TeF7t3Hr6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Eshglr76u-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc5de6c-3df2-453d-d4d4-9dd1f232b572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "BASE_PATH= '/content/drive/MyDrive/EXAMPLE_FOLDER/' #rename with your path\n",
        "\n",
        "#If you use Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cvgzjQSPvqn"
      },
      "source": [
        "# Library and tool"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports all necessary libraries"
      ],
      "metadata": {
        "id": "G51Y6ojE3fVy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lS0PYnHqgmdZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, matthews_corrcoef, f1_score, precision_score, recall_score\n",
        "from sklearn.ensemble import AdaBoostClassifier, HistGradientBoostingClassifier, VotingClassifier\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers import Input, Dense, Softmax, Layer, Concatenate, Activation, Conv2D, Flatten, DepthwiseConv2D, Lambda\n",
        "from keras.activations import softmax\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "from keras.regularizers import l2, l1\n",
        "from keras.metrics import AUC, Precision, Recall\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "np.int = np.int64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "install shap for XAI"
      ],
      "metadata": {
        "id": "qAZ1v6qR5iuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xai_method = 'shap',
        "!pip install shap==0.39.0\n",
        "import shap"
      ],
      "metadata": {
        "id": "1N8vZhjx5mpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data processing and split"
      ],
      "metadata": {
        "id": "Lw_LLqn-6aeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import the dataset and perform all necessary processing operations"
      ],
      "metadata": {
        "id": "JxylvFib_KLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4q2gVKXHf2Ky"
      },
      "outputs": [],
      "source": [
        "#Example\n",
        "df = pd.read_csv('/content/drive/MyDrive/Example_dataset.csv')\n",
        "\n",
        "#Split dataframe in train validation e test (50% - 30% -20%)\n",
        "target=\"column_name_target\"\n",
        "\n",
        "X_train, X_val = train_test_split(df,test_size=0.5, stratify=df[target], random_state=0, shuffle=True)\n",
        "X_val, X_test = train_test_split(X_val, test_size=0.4,stratify=X_val[target], random_state=0)\n",
        "\n",
        "\n",
        "#Convert to numpy\n",
        "y_train = X_train[target]\n",
        "X_train = X_train.drop([target], axis=1)\n",
        "\n",
        "y_val = X_val[target]\n",
        "X_val = X_val.drop(target, axis=1)\n",
        "\n",
        "y_test = X_test[target]\n",
        "X_test = X_test.drop(target, axis=1)\n",
        "\n",
        "X_train = X_train.to_numpy()\n",
        "y_train = y_train.to_numpy()\n",
        "\n",
        "X_val = X_val.to_numpy()\n",
        "y_val = y_val.to_numpy()\n",
        "\n",
        "X_test = X_test.to_numpy()\n",
        "y_test = y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orR_XFVHWKTx"
      },
      "source": [
        "#Models suite"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can enter and train as many models as you wish."
      ],
      "metadata": {
        "id": "Unxgy5tKKa2-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C16G17pCMIBP"
      },
      "source": [
        "##Model1 example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDvyStjMvPZI"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model1 =  RandomForestClassifier()   #LogisticRegression() , SVC(probability=True), DecisionTreeClassifier(), xgb.XGBClassifier() etc\n",
        "\n",
        "#train model\n",
        "model1.fit(X_train, y_train)\n",
        "\n",
        "#Evaluate hyperparameters on validation set\n",
        "y_proba = model1.predict_proba(X_val)\n",
        "y_pred = model1.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are satisfied with the results obtained on the validation set you can later use shap to get the XAI explanations on that portion of the data, save the model and save the model prediction probabilities"
      ],
      "metadata": {
        "id": "LwRjdCZvL63H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "X_shap, _, y_shap, _ = train_test_split(X_train,y_train,test_size=0.995, random_state=4, shuffle=True)\n",
        "explainer = shap.KernelExplainer(model1.predict_proba, X_shap)\n",
        "\n",
        "for i in tqdm(range(X_val.shape[0])):\n",
        "    shap_values = explainer.shap_values(X_val[i],nsamples=100)\n",
        "    to_append = shap_values[1].tolist()\n",
        "\n",
        "    results.append(to_append)\n",
        "\n",
        "#if the computation is slow, you can try decreasing number of samples\n",
        "#if you choose 0 as the explanation target, make the following changes.\n",
        "'''\n",
        "for i in tqdm(range(X_test.shape[0])):\n",
        "    #simply define expected_value=0\n",
        "    shap_values = explainer.shap_values(X_test[i], nsamples=100, expected_value=0)\n",
        "    to_append = shap_values[0].tolist()  #change index to 0\n",
        "\n",
        "    results.append(to_append)\n",
        "'''\n",
        "\n",
        "\n",
        "#save probability scores val set\n",
        "filename= BASE_PATH +'models/probas/'+model1+'_val.pickle'\n",
        "pickle.dump( y_proba, open(filename, 'wb'))\n",
        "\n",
        "#save explanation vectors val set\n",
        "filename = BASE_PATH + 'explainers/method=' + xai_method + '_val_' + model1 +'.pickle'\n",
        "pickle.dump(np.array(results), open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "7psfHF58LwTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remember that if you want you can retrain the model by merging validation with tran\n",
        "\n",
        "#Evaluate performance on Test set\n",
        "\n",
        "y_score = model1.predict_proba(X_test)\n",
        "y_pred = model1.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "#Compute explanation vectors also for the test set\n",
        "results = []\n",
        "\n",
        "X_shap, _, y_shap, _ = train_test_split(X_train,y_train,test_size=0.995, random_state=4, shuffle=True)\n",
        "explainer = shap.KernelExplainer(model1.predict_proba, X_shap)\n",
        "\n",
        "for i in tqdm(range(X_test.shape[0])):\n",
        "    if xai_method == 'shap':\n",
        "        shap_values = explainer.shap_values(X_test[i],nsamples=100)\n",
        "        to_append = shap_values[1].tolist()\n",
        "\n",
        "    results.append(to_append)\n",
        "\n",
        "\n",
        "#save probability scores test set\n",
        "filename= BASE_PATH + 'models/probas/'+model1+'_test.pickle'\n",
        "pickle.dump( y_score, open(filename, 'wb'))\n",
        "\n",
        "#save explanation vectors test set\n",
        "filename= BASE_PATH +'explainers_new/method='+ xai_method + '_test_' + model1 +'.pickle'\n",
        "pickle.dump(np.array(results), open(filename, 'wb'))\n",
        "\n",
        "#store the models\n",
        "with open(BASE_PATH + 'models/'+model1+'.pickle', 'wb') as f:\n",
        "    pickle.dump(model1, f)"
      ],
      "metadata": {
        "id": "PGa8Ll8fNnwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can replicate or modify this code to train as many models as you want"
      ],
      "metadata": {
        "id": "mu_xWUiFPd3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural newtwork for binary task"
      ],
      "metadata": {
        "id": "VRGlCkdDSpFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once all the explanations have been obtained from both the validation and the test set, it will be possible to reprocess this data so that it is usable to the network.\n",
        "Specifically, the explanations obtained from validation will serve as the train for the network; then in turn this data will be split into train and validation, while the test set portion will remain unchanged (In order to be able to perform a fair comparison)"
      ],
      "metadata": {
        "id": "zgdkLCVxUVcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##data preparation"
      ],
      "metadata": {
        "id": "_TxANsWbSwHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this data preparation is **only valid for binary tasks**."
      ],
      "metadata": {
        "id": "9SAmDiJ-S7J3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify the number of models you want to merge, the name of the models you want to merge, and the number of features the dataset has (excluding the target class)"
      ],
      "metadata": {
        "id": "iCS3Z4_HU713"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Num_model= 10                                    #number of models\n",
        "Num_feat = 8                                     #number of features\n",
        "nomi_modelli = ['model1','model2','model3']"
      ],
      "metadata": {
        "id": "bS2Trs8xS6gf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Vo11vHBM7t"
      },
      "source": [
        "##train, validation and test for the network"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we divide the dataset again to correctly associate the target value of the instances with the explanation vectors"
      ],
      "metadata": {
        "id": "cnwnhNMfWceG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example\n",
        "df = pd.read_csv('/content/drive/MyDrive/Dataset_Example.csv')\n",
        "\n",
        "#Split dataframe in train validation e test (50% - 30% -20%)\n",
        "target=\"column_name\"\n",
        "\n",
        "X_train, X_val = train_test_split(df,test_size=0.5, stratify=df[target], random_state=0, shuffle=True)\n",
        "X_val, X_test = train_test_split(X_val, test_size=0.4,stratify=X_val[target], random_state=0)"
      ],
      "metadata": {
        "id": "wAuvUOsPVvci"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BmGxZNZBzVY9"
      },
      "outputs": [],
      "source": [
        "nuovo_df = X_val  #the previous validation will serve as a train set for the network"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN"
      ],
      "metadata": {
        "id": "bhyVjvPYX-1p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iZrUcIdCLiP5"
      },
      "outputs": [],
      "source": [
        "#explanations are concatenated for each instance with probabilistic scores for each model\n",
        "dati = []\n",
        "\n",
        "for i in range(nuovo_df.shape[0]):\n",
        "    riga = []\n",
        "    ultimo_elemento = nuovo_df.iloc[i, -1]  #target\n",
        "\n",
        "    for nome_modello in nomi_modelli:\n",
        "        with open(BASE_PATH +'models/probas/'+nome_modello+'_val.pickle', 'rb') as f:\n",
        "            probas_modello = pickle.load(f)\n",
        "\n",
        "        with open( BASE_PATH +'explainers/method=shap_val_'+nome_modello+'.pickle', 'rb') as f:\n",
        "            shap_val_modello = pickle.load(f)\n",
        "\n",
        "        riga += [probas_modello[i][1]] + list(shap_val_modello[i])\n",
        "\n",
        "    riga += [ultimo_elemento]\n",
        "    dati.append(riga)\n",
        "\n",
        "train_df = pd.DataFrame(dati)\n",
        "print(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ToZdR3hiEBf"
      },
      "source": [
        "TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62BUbrmTiH7P"
      },
      "outputs": [],
      "source": [
        "#in the same way for the test set portion\n",
        "dati = []\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "    riga=[]\n",
        "    ultimo_elemento = X_test.iloc[i, -1]  #target\n",
        "\n",
        "    for nome_modello in nomi_modelli:\n",
        "        with open(BASE_PATH + 'models/probas/'+nome_modello+'_test.pickle', 'rb') as f:\n",
        "            probas_modello = pickle.load(f)\n",
        "\n",
        "        with open(BASE_PATH + 'explainers/method=shap_test_'+nome_modello+'.pickle', 'rb') as f:\n",
        "            shap_val_modello = pickle.load(f)\n",
        "\n",
        "        riga += [probas_modello[i][1]] + list(shap_val_modello[i])\n",
        "\n",
        "    riga += [ultimo_elemento]\n",
        "    dati.append(riga)\n",
        "\n",
        "test_df = pd.DataFrame(dati)\n",
        "print(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VALIDATION"
      ],
      "metadata": {
        "id": "RRgvZd9mYf45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now extract from the new train set a portion of data that will serve as the validation set for the network"
      ],
      "metadata": {
        "id": "TQdVL5qKYqPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UqobM1zQ-5L"
      },
      "outputs": [],
      "source": [
        "X_train, X_val = train_test_split(train_df,test_size=0.3, stratify=train_df.iloc[:,-1], random_state=4, shuffle=True)  #30% of train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkxH-6EEd4sT"
      },
      "source": [
        "##creation of explanation matrices and score vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within this section, explanation matrices and scores vectors will be created to feed to the network. Since the network will use a train, validation, and later be evaluated on a test set, it will be necessary to create 3 matrices and 3 vectors, each associated with its portion of the dataset"
      ],
      "metadata": {
        "id": "mYCum17pZ6Xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   TRAIN: scores_t and expl_t\n",
        "*   VALIDATION: scores_v and expl_v\n",
        "*   TEST: scores_te and expl_te"
      ],
      "metadata": {
        "id": "c2NlehgsZ7vw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcza4OEFd4st"
      },
      "source": [
        "X TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9Rry2F_d4sv"
      },
      "outputs": [],
      "source": [
        "scores_t = np.zeros((X_train.shape[0],Num_model)) #define score vector\n",
        "y_train = X_train.iloc[:,-1]\n",
        "X_train = X_train.iloc[:, :-1]\n",
        "y_train = y_train.to_numpy()\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    scores = [X_train.iloc[i, 0]]\n",
        "    for j in range(1, Num_model):\n",
        "      scores.append(X_train.iloc[i, (Num_feat + 1) * j])\n",
        "    scores_t[i,:] = scores\n",
        "\n",
        "expl_t = np.zeros((X_train.shape[0],Num_model,Num_feat)) #define explanation matrix\n",
        "\n",
        "columns_to_drop = [0] + [(Num_feat + 1) * i for i in range(1, Num_model)]\n",
        "X_train = X_train.drop(columns=X_train.columns[columns_to_drop])\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  riga = X_train.iloc[i].values\n",
        "  parti = np.array_split(riga,Num_model)\n",
        "  for j in range(Num_model):\n",
        "    expl_t[i,j,:]= parti[j]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRm_D3NGd4sy"
      },
      "source": [
        "X VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smWnPK_Id4sz"
      },
      "outputs": [],
      "source": [
        "scores_v = np.zeros((X_val.shape[0],Num_model))\n",
        "y_val = X_val.iloc[:,-1]\n",
        "X_val = X_val.iloc[:, :-1]\n",
        "y_val = y_val.to_numpy()\n",
        "\n",
        "for i in range(len(X_val)):\n",
        "    scores = [X_val.iloc[i, 0]]\n",
        "    for j in range(1, Num_model):\n",
        "      scores.append(X_val.iloc[i, (Num_feat + 1) * j])\n",
        "    scores_v[i,:] = scores\n",
        "\n",
        "expl_v = np.zeros((X_val.shape[0],Num_model,Num_feat))\n",
        "\n",
        "columns_to_drop = [0] + [(Num_feat + 1) * i for i in range(1, Num_model)]\n",
        "X_val = X_val.drop(columns=X_val.columns[columns_to_drop])\n",
        "\n",
        "for i in range(len(X_val)):\n",
        "  riga = X_val.iloc[i].values\n",
        "  parti = np.array_split(riga,Num_model)\n",
        "  for j in range(Num_model):\n",
        "    expl_v[i,j,:]= parti[j]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q0RsQKGd4s1"
      },
      "source": [
        "X TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwzqQSH4d4s2"
      },
      "outputs": [],
      "source": [
        "df= test_df\n",
        "scores_te = np.zeros((df.shape[0],Num_model))\n",
        "y_test = df.iloc[:,-1]\n",
        "df = df.iloc[:, :-1]\n",
        "y_test = y_test.to_numpy()\n",
        "\n",
        "for i in range(len(df)):\n",
        "    scores = [df.iloc[i, 0]]\n",
        "    for j in range(1, Num_model):\n",
        "      scores.append(df.iloc[i, (Num_feat + 1) * j])\n",
        "    scores_te[i,:] = scores\n",
        "\n",
        "expl_te = np.zeros((df.shape[0],Num_model,Num_feat))\n",
        "\n",
        "columns_to_drop = [0] + [(Num_feat + 1) * i for i in range(1, Num_model)]\n",
        "df = df.drop(columns=df.columns[columns_to_drop])\n",
        "\n",
        "for i in range(len(df)):\n",
        "  riga = df.iloc[i].values\n",
        "  parti = np.array_split(riga,Num_model)\n",
        "  for j in range(Num_model):\n",
        "    expl_te[i,j,:]= parti[j]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural network architecture and training"
      ],
      "metadata": {
        "id": "bWfbfTJ6puZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not being present in scikit learn the mcc, it is necessary to define a custom metric. If you do not want to use this metric you can skip this step"
      ],
      "metadata": {
        "id": "2IK_euVrqY2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mcc(y_true, y_pred):\n",
        "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "    y_pred_neg = 1 - y_pred_pos\n",
        "\n",
        "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "    y_neg = 1 - y_pos\n",
        "\n",
        "    tp = K.sum(y_pos * y_pred_pos)\n",
        "    tn = K.sum(y_neg * y_pred_neg)\n",
        "\n",
        "    fp = K.sum(y_neg * y_pred_pos)\n",
        "    fn = K.sum(y_pos * y_pred_neg)\n",
        "\n",
        "    numerator = (tp * tn - fp * fn)\n",
        "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "\n",
        "    return numerator / (denominator + K.epsilon())"
      ],
      "metadata": {
        "id": "7zgXRzhJp1gE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify and edit hyperparameters"
      ],
      "metadata": {
        "id": "XEvl0dAUzr3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "\n",
        "lr= 0.0001                                        #learning rate\n",
        "momentum = 0.80                                   #momentum\n",
        "Dim_layer= 32                                     #number units in hidden layer\n",
        "act_func_hiddenlayer='relu'                       #activation function hidden layer\n",
        "act_func_weightslayer='relu'                      #activation function weights layer\n",
        "value_reg = 0.01                                  #value for kernel regularization l1 or l2\n",
        "opt = Adam(learning_rate=lr, beta_1=momentum)     #Optimizer example, Adam\n",
        "funct_loss = 'binary_crossentropy'                #loss function, example for binary task\n",
        "monitor_metrics = ['binary_accuracy','AUC',mcc]   #metric to visualize\n",
        "num_ep= 4000                                      #number of epocs\n",
        "batch_s = 62                                      #batch size"
      ],
      "metadata": {
        "id": "_QuC-cV5uPbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we move on to the creation of the network using the Keras library"
      ],
      "metadata": {
        "id": "O87XUjyQqmDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define input\n",
        "scores_in = Input(shape=(Num_model), name='scores_in')\n",
        "expl_in = Input(shape=(Num_model, Num_feat,1), name='expl_in')\n",
        "\n",
        "#Convolution Layer\n",
        "conv = Conv2D(1, (15,15),padding='same')(expl_in)   #need 4 dimension (batch_size, h, l, channels)\n",
        "\n",
        "#flatten result convolution\n",
        "flattened_conv = Flatten()(conv)\n",
        "\n",
        "#concatenates the two inputs\n",
        "conc = Concatenate()([scores_in,flattened_conv])\n",
        "\n",
        "#Hidden layer Dense (fully connected)\n",
        "layer_z = Dense(Dim_layer, activation=act_func_hiddenlayer,name='hidden_layer', kernel_regularizer=l2(value_reg))(conc) #Specify L1 or L2 regulato\n",
        "\n",
        "#Weights layer\n",
        "weights = Dense(Num_model,activation=act_func_weightslayer,name='weights_layer')(layer_z)\n",
        "\n",
        "#Normalize weights\n",
        "weights_n =Activation('softmax')(weights)\n",
        "\n",
        "#output finale\n",
        "sum= tf.reduce_sum(tf.multiply(weights,scores_in),axis=1)\n",
        "\n",
        "#execute weighted sum\n",
        "model = Model(inputs=[scores_in,expl_in], outputs= sum)\n",
        "\n",
        "#specifies metrics from monitor, optimizer and the loss function\n",
        "model.compile(loss=funct_loss, optimizer=opt, metrics=['binary_accuracy','AUC',mcc])\n",
        "\n",
        "#network summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "AkIEeRjqqmME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def early stopping technique, by monitoring validation loss\n",
        "early_stopping = EarlyStopping(monitor='val_loss',mode='min', patience=100, restore_best_weights=True)\n",
        "\n",
        "#Train model\n",
        "history = model.fit([scores_t,expl_t], y_train,batch_size=batch_s, epochs=num_ep, validation_data=([scores_v,expl_v], y_val), callbacks=[early_stopping])\n",
        "\n",
        "#Plot plots the graphs\n",
        "# plot training history loss\n",
        "plt.title(\"Loss \")\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "#and others if you want\n",
        "'''\n",
        "#acc\n",
        "plt.title(\"Accuracy \")\n",
        "plt.plot(history.history['binary_accuracy'], label='train accuracy')\n",
        "plt.plot(history.history['val_binary_accuracy'], label='val accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "#auc\n",
        "plt.title(\"Roc AUC score \")\n",
        "plt.plot(history.history['auc'], label='auc train')\n",
        "plt.plot(history.history['val_auc'], label='auc val')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Auc')\n",
        "plt.show()\n",
        "\n",
        "#mcc\n",
        "plt.title(\"MCC \")\n",
        "plt.plot(history.history['mcc'], label='mcc train')\n",
        "plt.plot(history.history['val_mcc'], label='mcc val')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MCC')\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "hL5iwFhKuEfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once finished, perform the final evaluation on the test set"
      ],
      "metadata": {
        "id": "M--BWz81yGPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "y =model.predict([scores_te,expl_te])\n",
        "y_pred = [0 if x <= 0.5 else 1 for x in y]  #since the output is probabilistic\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "MsbItC8cyGbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#store or load your model\n",
        "#model.save(BASE_PATH+'Explusion_network.keras')\n",
        "#model = load_model(BASE_PATH+ 'Explusion_network.keras',custom_objects={'mcc': mcc} )"
      ],
      "metadata": {
        "id": "No7wGKG4yrKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
